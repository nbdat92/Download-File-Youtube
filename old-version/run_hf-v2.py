#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#T·∫£i->convert->ƒë·∫©y->x√≥a

import sys
import os
import re
import time
from pathlib import Path
from typing import List, Optional, Tuple, Dict

from yt_dlp import YoutubeDL
from huggingface_hub import HfApi, create_repo, hf_hub_url, upload_file

# ---------- Config loader: TOML (py311+ d√πng tomllib; th·∫•p h∆°n d√πng 'toml') ----------
def load_toml(path: Path) -> dict:
    if not path.exists():
        return {}
    try:
        import tomllib  # Py 3.11+
        return tomllib.loads(path.read_text(encoding="utf-8"))
    except ModuleNotFoundError:
        import toml  # pip install toml (n·∫øu <3.11)
        return toml.loads(path.read_text(encoding="utf-8"))

# ---------- ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh ----------
ROOT = Path.cwd()
CONF_FILE = ROOT / "run_hf.toml"
DOWNLOAD_DIR = ROOT / "downloads"
DOWNLOAD_DIR.mkdir(exist_ok=True)
LINK_FILE = ROOT / "link.txt"

# ---------- Helper in/out ----------
def parse_cli(argv: List[str]) -> Tuple[Dict[str, str], List[str]]:
    """
    Tr·∫£ v·ªÅ (overrides, cli_urls). overrides ch·ª©a c√°c tr∆∞·ªùng ng∆∞·ªùi d√πng truy·ªÅn b·∫±ng CLI.
    """
    ov: Dict[str, str] = {}
    urls: List[str] = []
    i = 1
    while i < len(argv):
        a = argv[i]
        if a in ("-h", "--help"):
            print_help_and_exit()
        if a == "--hf-repo-id":
            ov["hf.repo_id"] = argv[i+1]; i += 2; continue
        if a == "--hf-repo-type":
            ov["hf.repo_type"] = argv[i+1]; i += 2; continue
        if a == "--hf-branch":
            ov["hf.branch"] = argv[i+1]; i += 2; continue
        if a == "--hf-path-prefix":
            ov["hf.path_prefix"] = argv[i+1]; i += 2; continue
        if a == "--cookies":
            ov["cookies.path"] = argv[i+1]; i += 2; continue
        urls.append(a); i += 1
    return ov, urls

def print_help_and_exit():
    print(rf"""
YouTube -> Hugging Face Uploader  (config-first, per-item upload)

C√°ch d√πng t·ªëi gi·∫£n:
  python {Path(__file__).name}

∆Øu ti√™n c·∫•u h√¨nh (cao -> th·∫•p):
  CLI (--hf-..., --cookies)  >  ENV (HF_TOKEN, YT_COOKIES)  >  run_hf.toml  >  m·∫∑c ƒë·ªãnh

Tu·ª≥ ch·ªçn CLI (ghi ƒë√® config):
  --hf-repo-id USER/REPO
  --hf-repo-type dataset|model|space
  --hf-branch main
  --hf-path-prefix mp4/
  --cookies path/to/cookies.txt

ENV h·ªó tr·ª£:
  HF_TOKEN, HF_REPO_ID, HF_REPO_TYPE, HF_BRANCH, HF_PATH_PREFIX, YT_COOKIES

Link ƒë·∫ßu v√†o:
  - Tham s·ªë CLI (URL1 URL2 ...)
  - Ho·∫∑c file link.txt (m·ªói d√≤ng 1 URL)
  - Ho·∫∑c d√°n tay khi ch·∫°y

Ch·ªçn ƒë·∫ßu ra:
  1) MP4 (best)  2) MP3  3) WAV

Config file: run_hf.toml (ƒë·∫∑t c·∫°nh script)
""")
    sys.exit(0)

def merge_config(cli_ov: Dict[str, str], conf: dict) -> dict:
    """H·ª£p nh·∫•t theo ∆∞u ti√™n: CLI > ENV > TOML > default"""
    # 1) B·∫Øt ƒë·∫ßu t·ª´ TOML
    merged = {
        "hf": {
            "token": conf.get("hf", {}).get("token", "").strip(),
            "repo_id": conf.get("hf", {}).get("repo_id", "").strip(),
            "repo_type": conf.get("hf", {}).get("repo_type", "dataset").strip() or "dataset",
            "branch": conf.get("hf", {}).get("branch", "main").strip() or "main",
            "path_prefix": conf.get("hf", {}).get("path_prefix", "").strip(),
        },
        "cookies": {
            "path": conf.get("cookies", {}).get("path", "").strip(),
        },
        "downloader": {
            "ratelimit": int(conf.get("downloader", {}).get("ratelimit", 2_000_000)),
            "sleep_interval": float(conf.get("downloader", {}).get("sleep_interval", 2)),
            "max_sleep_interval": float(conf.get("downloader", {}).get("max_sleep_interval", 5)),
            "sleep_requests": float(conf.get("downloader", {}).get("sleep_requests", 0.5)),
        }
    }

    # 2) ENV ghi ƒë√® TOML (n·∫øu c√≥)
    env_overrides = {
        "hf.token": os.getenv("HF_TOKEN", "").strip(),
        "hf.repo_id": os.getenv("HF_REPO_ID", "").strip(),
        "hf.repo_type": os.getenv("HF_REPO_TYPE", "").strip(),
        "hf.branch": os.getenv("HF_BRANCH", "").strip(),
        "hf.path_prefix": os.getenv("HF_PATH_PREFIX", "").strip(),
        "cookies.path": os.getenv("YT_COOKIES", "").strip(),
    }
    for k, v in env_overrides.items():
        if v:
            sect, key = k.split(".")
            merged[sect][key] = v

    # 3) CLI ghi ƒë√® t·∫•t c·∫£
    for k, v in cli_ov.items():
        sect, key = k.split(".")
        merged[sect][key] = v

    return merged

def choose_mode() -> str:
    print("\nCh·ªçn ch·∫ø ƒë·ªô xu·∫•t:")
    print("  1) Video MP4 (ch·∫•t l∆∞·ª£ng cao nh·∫•t)")
    print("  2) √Çm thanh MP3")
    print("  3) √Çm thanh WAV")
    while True:
        c = input("Nh·∫≠p 1 / 2 / 3: ").strip()
        if c in {"1", "2", "3"}:
            return c
        print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")

def collect_urls(cli_urls: List[str]) -> List[str]:
    if cli_urls:
        return cli_urls
    if LINK_FILE.exists():
        print(f"ƒê·ªçc link t·ª´ {LINK_FILE} ...")
        urls = [ln.strip() for ln in LINK_FILE.read_text(encoding="utf-8").splitlines() if ln.strip()]
        if urls: return urls
    print("Nh·∫≠p 1 ho·∫∑c nhi·ªÅu link (c√°ch nhau b·ªüi kho·∫£ng tr·∫Øng / xu·ªëng d√≤ng). K·∫øt th√∫c b·∫±ng d√≤ng tr·ªëng:")
    buf = []
    while True:
        try:
            line = input().strip()
        except EOFError:
            break
        if not line: break
        buf += re.split(r"\s+", line)
    urls = [u for u in buf if u]
    if not urls:
        print("Kh√¥ng c√≥ URL n√†o. Tho√°t."); sys.exit(1)
    return urls

# ---------- yt-dlp ----------
def progress_hook(d):
    if d.get("status") == "downloading":
        p = d.get("_percent_str", "").strip()
        spd = d.get("speed")
        eta = d.get("eta")
        print(f"ƒêang t·∫£i: {p:<6} | T·ªëc ƒë·ªô: {spd or '-':<10} | ETA: {eta or '-'}", end="\r", flush=True)
    elif d.get("status") == "finished":
        print("\n‚úì T·∫£i xong, ƒëang x·ª≠ l√Ω (ffmpeg)...")

def make_opts(mode: str, cookies_path: Optional[str], dl_cfg: dict):
    common = {
        "outtmpl": str(DOWNLOAD_DIR / "%(title)s [%(id)s].%(ext)s"),
        "ignoreerrors": True,
        "noplaylist": False,
        "concurrent_fragment_downloads": 4,
        "retries": 10,
        "fragment_retries": 10,
        "http_chunk_size": 10 * 1024 * 1024,
        "progress_hooks": [progress_hook],
        "trim_file_name": 240,
        "quiet": False,
        "no_warnings": True,
        # l·ªãch s·ª±
        "ratelimit": dl_cfg.get("ratelimit", 2_000_000),
        "sleep_interval": dl_cfg.get("sleep_interval", 2),
        "max_sleep_interval": dl_cfg.get("max_sleep_interval", 5),
        "sleep_requests": dl_cfg.get("sleep_requests", 0.5),
    }
    if cookies_path:
        common["cookiefile"] = cookies_path

    if mode == "1":
        return {
            **common,
            "format": ("bestvideo[ext=mp4][vcodec*=avc]/bestvideo[vcodec*=avc]+bestaudio[ext=m4a]/"
                       "best[ext=mp4]/best"),
            "merge_output_format": "mp4",
            "postprocessors": [
                {"key": "FFmpegVideoConvertor", "preferedformat": "mp4"},
                {"key": "FFmpegMetadata"},
            ],
        }
    # audio
    target = "mp3" if mode == "2" else "wav"
    return {
        **common,
        "format": "bestaudio/best",
        "postprocessors": [
            {"key": "FFmpegExtractAudio", "preferredcodec": target, "preferredquality": "0"},
            {"key": "FFmpegMetadata"},
        ],
        "prefer_ffmpeg": True,
    }

# ---------- HF ----------
def ensure_hf_repo(api: HfApi, token: str, repo_id: str, repo_type: str):
    create_repo(repo_id=repo_id, repo_type=repo_type, token=token, exist_ok=True)

def hf_upload(api: HfApi, token: str, repo_id: str, repo_type: str, branch: str, fpath: Path, path_in_repo: str) -> str:
    upload_file(
        path_or_fileobj=str(fpath),
        path_in_repo=path_in_repo,
        repo_id=repo_id,
        repo_type=repo_type,
        token=token,
        revision=branch,
    )
    return hf_hub_url(repo_id=repo_id, filename=path_in_repo, repo_type=repo_type, revision=branch)

def infer_path_in_repo(prefix: str, name: str) -> str:
    prefix = (prefix or "").strip().lstrip("/")
    return f"{prefix}/{name}" if prefix else name

# ---------- Orchestrate (per-item: download -> upload -> delete) ----------
def run_pipeline(urls: List[str], mode: str, cfg: dict):
    hf = cfg["hf"]; cookies = cfg["cookies"]; dl_cfg = cfg["downloader"]
    token = hf.get("token") or os.getenv("HF_TOKEN", "").strip()
    if not token:
        print("‚ùå Thi·∫øu HF token (ƒëi·ªÅn v√†o run_hf.toml [hf].token ho·∫∑c ENV HF_TOKEN)."); sys.exit(2)
    repo_id = hf.get("repo_id") or os.getenv("HF_REPO_ID", "")
    if not repo_id:
        print("‚ùå Thi·∫øu repo_id (ƒëi·ªÅn [hf].repo_id ho·∫∑c ENV HF_REPO_ID)."); sys.exit(2)
    repo_type = hf.get("repo_type") or os.getenv("HF_REPO_TYPE", "dataset")
    branch = hf.get("branch") or os.getenv("HF_BRANCH", "main")
    prefix = hf.get("path_prefix") or os.getenv("HF_PATH_PREFIX", "")

    cookies_path = cookies.get("path") or os.getenv("YT_COOKIES", "")
    if cookies_path and not Path(cookies_path).exists():
        print(f"‚ö†Ô∏è  Cookies kh√¥ng t·ªìn t·∫°i: {cookies_path}. B·ªè qua.")
        cookies_path = None
    elif not cookies_path:
        cookies_path = None

    ydl_opts = make_opts(mode, cookies_path, dl_cfg)

    kind = "MP4" if mode == "1" else ("MP3" if mode == "2" else "WAV")
    print("\n======== TH√îNG TIN T√ÅC V·ª§ ========")
    print("ƒê·∫ßu ra    :", kind)
    print("Local     :", DOWNLOAD_DIR.resolve(), "(t·∫°m)")
    print("Cookies   :", cookies_path or "(kh√¥ng d√πng)")
    print("HF repo   :", repo_id, f"({repo_type})")
    print("HF branch :", branch)
    print("HF prefix :", prefix or "(root)")
    print("S·ªë link   :", len(urls))
    print("===================================\n")

    api = HfApi()
    ensure_hf_repo(api, token, repo_id, repo_type)

    # ---- L·∫∑p t·ª´ng URL: t·∫£i -> ƒë·∫©y ngay -> xo√° file local ----
    with YoutubeDL(ydl_opts) as ydl:
        for i, url in enumerate(urls, 1):
            print(f"\n----- [{i}/{len(urls)}] {url}")

            # Ghi nh·∫≠n danh s√°ch file hi·ªán c√≥, ƒë·ªÉ sau khi t·∫£i xong x√°c ƒë·ªãnh ƒë√∫ng file m·ªõi sinh
            before = {p.resolve() for p in DOWNLOAD_DIR.glob("*")}

            try:
                ydl.download([url])
            except Exception as e:
                msg = str(e).lower()
                print(f"‚ùå L·ªói t·∫£i: {e}")
                if "429" in msg or "too many requests" in msg or "http error 403" in msg:
                    print("‚ö†Ô∏è  Rate-limit nghi ng·ªù. Ngh·ªâ 5 ph√∫t r·ªìi ti·∫øp...")
                    time.sleep(300)
                continue

            # ƒê·ª£i r·∫•t ng·∫Øn cho postprocessor (ffmpeg) flush file ra ƒëƒ©a
            time.sleep(0.5)

            # X√°c ƒë·ªãnh c√°c file m·ªõi (sau convert)
            after = {p.resolve() for p in DOWNLOAD_DIR.glob("*")}
            new_files = [p for p in sorted(after - before, key=lambda x: x.stat().st_mtime) if p.is_file()]
            if not new_files:
                print("‚ö†Ô∏è  Kh√¥ng th·∫•y file m·ªõi sau khi t·∫£i/convert.")
                continue

            # V·ªõi 1 URL YouTube, th∆∞·ªùng sinh 1 file cu·ªëi (MP4 ho·∫∑c MP3/WAV). Nh∆∞ng n·∫øu playlist-item
            # th√¨ yt-dlp v·∫´n ƒëi t·ª´ng item; ·ªü ƒë√¢y c·ª© x·ª≠ l√Ω t·∫•t c·∫£ file m·ªõi (hi·∫øm khi tr√πng th·ªùi ƒëi·ªÉm).
            for f in new_files:
                dst = infer_path_in_repo(prefix, f.name)
                try:
                    print(f"‚Üë Upload HF: {dst} ...")
                    url_file = hf_upload(api, token, repo_id, repo_type, branch, f, dst)
                    print(f"   ‚úì {url_file}")
                except Exception as up_e:
                    print(f"   ‚ùå L·ªói upload: {up_e}")
                    continue

                # Xo√° local NGAY khi upload xong
                try:
                    f.unlink()
                    print(f"   üßπ Xo√° local: {f.name}")
                except Exception as del_e:
                    print(f"   ‚ö†Ô∏è Kh√¥ng xo√° ƒë∆∞·ª£c {f.name}: {del_e}")

    print("\n‚úÖ Ho√†n t·∫•t to√†n b·ªô danh s√°ch (ƒë√£ upload t·ª´ng m·ª•c & d·ªçn file t·∫°m).")

# ---------- main ----------
def main():
    cli_ov, cli_urls = parse_cli(sys.argv)
    conf = load_toml(CONF_FILE)
    cfg = merge_config(cli_ov, conf)

    urls = collect_urls(cli_urls)
    mode = choose_mode()
    run_pipeline(urls, mode, cfg)

if __name__ == "__main__":
    main()
